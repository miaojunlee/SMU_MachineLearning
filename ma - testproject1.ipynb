{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"E:/python/testproject/20180621_LLM_with_GPS_train.xlsx\")\n",
    "df.columns = [c.replace(' ', '_') for c in df.columns]\n",
    "df.columns = [c.replace('(', '') for c in df.columns]\n",
    "df.columns = [c.replace(')', '') for c in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_droped = df[np.logical_not(pd.isna(df[\"completed_time\"]))]\n",
    "df_droped = df_droped[np.logical_not(pd.isna(df_droped[\"driver_taken_time\"]))]\n",
    "df_droped = df_droped[np.logical_not(pd.isna(df_droped[\"pickedup_time\"]))]\n",
    "#df_droped = df[df.order_type == \"Immediate\"]\n",
    "#df_droped = df_droped[df_droped.order_status == \"Completed\"]\n",
    "#df_droped = df_droped[df_droped.service_type == \"VAN\"]\n",
    "\n",
    "countcomma = lambda x: x.count(\",\") - 1\n",
    "df_droped[\"pathstops\"] = list(map(countcomma, df_droped.path.astype(\"str\")))\n",
    "#df_droped = df_droped[df_droped[\"pathcomma\"] == 1]\n",
    "\n",
    "df_droped[\"path_from\"] = df_droped.path.str.split(\",\", 1, expand=True).iloc[:,0]\n",
    "df_droped[\"path_to\"] = df_droped.path.str.rsplit(\",\", 1, expand=True).iloc[:,1]\n",
    "\n",
    "#df_path = df_droped.path.str.split(\",\", expand=True)\n",
    "#df_path.columns = [\"path_from\",\"path_to\"]\n",
    "#df_droped = pd.concat([df_droped, df_path], axis = 1)\n",
    "# sum(pd.isna(df_droped[\"completed_time\"]) + 0)\n",
    "# still two missing, will drop\n",
    "\n",
    "#df_droped[\"tips\"] = df_droped[\"total_price_+_tips\"] - df_droped[\"total_price\"]\n",
    "#df_droped[\"total_adj\"] = df_droped[\"transaction_value\"] - df_droped[\"total_price\"]\n",
    "\n",
    "df_droped[\"placed_year\"] = df_droped[\"placed_time\"].dt.year\n",
    "df_droped[\"placed_month\"] = df_droped[\"placed_time\"].dt.month\n",
    "df_droped[\"placed_weekday\"] = df_droped[\"placed_time\"].dt.weekday\n",
    "df_droped[\"placed_hour\"] = df_droped[\"placed_time\"].dt.hour\n",
    "df_droped[\"placed_minute\"] = df_droped[\"placed_time\"].dt.minute\n",
    "\n",
    "df_droped[\"taken_minutes\"] = (df_droped[\"driver_taken_time\"] - df_droped[\"placed_time\"]).astype('timedelta64[m]')\n",
    "df_droped[\"pickup_minutes\"] = (df_droped[\"pickedup_time\"] - df_droped[\"placed_time\"]).astype('timedelta64[m]')\n",
    "\n",
    "#df_droped[\"latitude_dis\"] = abs(df_droped[\"driver_latitude_pick_up\"] - df_droped[\"driver_latitude_complete\"])\n",
    "#df_droped[\"longitude_dis\"] = abs(df_droped[\"driver_longitude_pick_up\"] - df_droped[\"driver_longitude_complete\"])\n",
    "\n",
    "#columns_to_cut = [\"driver_latitude_pick_up\", \"driver_longitude_pick_up\", \"driver_latitude_complete\", \"driver_longitude_complete\"]\n",
    "\n",
    "#for i in columns_to_cut:\n",
    "#    df_droped[i] = pd.qcut(df_droped[i],50)\n",
    "    \n",
    "df_droped[\"placed_minute\"] = pd.qcut(df_droped[\"placed_minute\"],6)\n",
    "    \n",
    "#impute na with 0 or mean for some continues\n",
    "#df_droped[[\"subsidy\", \"adjustment\"]] = df_droped[[\"subsidy\", \"adjustment\"]].fillna(0)\n",
    "#df_droped[\"dis\"] = np.sqrt(df_droped[\"latitude_dis\"]**2 + df_droped[\"longitude_dis\"]**2)\n",
    "#df_droped.loc[df_droped[\"dis\"] >1, \"dis\"] = np.nan\n",
    "#df_droped[\"dis\"] = df_droped[\"dis\"].fillna(np.mean(df_droped[\"dis\"]))\n",
    "\n",
    "df_droped[\"taken_minutes\"] = df_droped[\"taken_minutes\"].fillna(np.mean(df_droped[\"taken_minutes\"]))\n",
    "df_droped[\"pickup_minutes\"] = df_droped[\"pickup_minutes\"].fillna(np.mean(df_droped[\"pickup_minutes\"]))\n",
    "\n",
    "df_droped[\"target\"] = (df_droped.completed_time - df_droped.placed_time)/np.timedelta64(1,'h') > 1\n",
    "\n",
    "df_droped = df_droped.drop([\"order_status\", \"order_date_time\", \"cs_comment\", \"cancel_type\", \"cancel_reason\", \"cancelled_by\",\n",
    "              \"cancelled_time\", \"is_responded\", \"reject_time\", \"arrival_time_geofence\",\n",
    "              \"driver_latitude_take_order\", \"driver_longitude_take_order\", \"path\", \n",
    "              \"completed_time\", \"placed_time\",\"total_price_+_tips\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tmp = pd.read_excel(\"E:/python/testproject/LLM_test.xlsx\")\n",
    "df_tmp.columns = [c.replace(' ', '_') for c in df_tmp.columns]\n",
    "df_tmp.columns = [c.replace('(', '') for c in df_tmp.columns]\n",
    "df_tmp.columns = [c.replace(')', '') for c in df_tmp.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test_droped = df_tmp.drop([\"order_status\"], axis = 1)\n",
    "time_variables = [\"placed_time\", \"driver_taken_time\", \"pickedup_time\"]\n",
    "for v in time_variables:\n",
    "    df_test_droped[v] = pd.to_datetime(df_test_droped[v])\n",
    "    \n",
    "df_test_droped[\"special_request\"] = df_test_droped[\"order_remarks\"]\n",
    "#df_test_droped = df[df.order_type == \"Immediate\"]\n",
    "#df_test_droped = df_test_droped[df_test_droped.order_status == \"Completed\"]\n",
    "#df_test_droped = df_test_droped[df_test_droped.service_type == \"VAN\"]\n",
    "\n",
    "countcomma = lambda x: x.count(\",\") - 1\n",
    "df_test_droped[\"pathstops\"] = list(map(countcomma, df_test_droped.path.astype(\"str\")))\n",
    "#df_test_droped = df_test_droped[df_test_droped[\"pathcomma\"] == 1]\n",
    "\n",
    "df_test_droped[\"path_from\"] = df_test_droped.path.str.split(\",\", 1, expand=True).iloc[:,0]\n",
    "df_test_droped[\"path_to\"] = df_test_droped.path.str.rsplit(\",\", 1, expand=True).iloc[:,1]\n",
    "\n",
    "#df_path = df_test_droped.path.str.split(\",\", expand=True)\n",
    "#df_path.columns = [\"path_from\",\"path_to\"]\n",
    "#df_test_droped = pd.concat([df_test_droped, df_path], axis = 1)\n",
    "# sum(pd.isna(df_test_droped[\"completed_time\"]) + 0)\n",
    "# still two missing, will drop\n",
    "\n",
    "#df_test_droped[\"tips\"] = df_test_droped[\"total_price_+_tips\"] - df_test_droped[\"total_price\"]\n",
    "#df_test_droped[\"total_adj\"] = df_test_droped[\"transaction_value\"] - df_test_droped[\"total_price\"]\n",
    "\n",
    "df_test_droped[\"placed_year\"] = df_test_droped[\"placed_time\"].dt.year\n",
    "df_test_droped[\"placed_month\"] = df_test_droped[\"placed_time\"].dt.month\n",
    "df_test_droped[\"placed_weekday\"] = df_test_droped[\"placed_time\"].dt.weekday\n",
    "df_test_droped[\"placed_hour\"] = df_test_droped[\"placed_time\"].dt.hour\n",
    "df_test_droped[\"placed_minute\"] = df_test_droped[\"placed_time\"].dt.minute\n",
    "df_test_droped[\"placed_minute\"] = pd.qcut(df_test_droped[\"placed_minute\"], 6)\n",
    "\n",
    "df_test_droped[\"taken_minutes\"] = (df_test_droped[\"driver_taken_time\"] - df_test_droped[\"placed_time\"]).astype('timedelta64[m]')\n",
    "df_test_droped[\"pickup_minutes\"] = (df_test_droped[\"pickedup_time\"] - df_test_droped[\"placed_time\"]).astype('timedelta64[m]')\n",
    "\n",
    "df_test_droped[\"taken_minutes\"] = df_test_droped[\"taken_minutes\"].fillna(np.mean(df_test_droped[\"taken_minutes\"]))\n",
    "df_test_droped[\"pickup_minutes\"] = df_test_droped[\"pickup_minutes\"].fillna(np.mean(df_test_droped[\"pickup_minutes\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_test_droped = df_tmp[df_tmp.order_type == \"Immediate\"]\n",
    "df_test_droped = df_test_droped[df_test_droped.order_status == \"Completed\"]\n",
    "df_test_droped = df_test_droped[df_test_droped.service_type == \"VAN\"]\n",
    "\n",
    "countcomma = lambda x: x.count(\",\")\n",
    "df_test_droped[\"pathcomma\"] = list(map(countcomma, df_test_droped.path))\n",
    "df_test_droped = df_test_droped[df_test_droped[\"pathcomma\"] == 1]\n",
    "\n",
    "df_path = df_test_droped.path.str.split(\",\", expand=True)\n",
    "df_path.columns = [\"path_from\",\"path_to\"]\n",
    "df_test_droped = pd.concat([df_test_droped, df_path], axis = 1)\n",
    "\n",
    "df_test_droped[\"tips\"] = df_test_droped[\"total_price_+_tips\"] - df_test_droped[\"total_price\"]\n",
    "df_test_droped[\"total_adj\"] = df_test_droped[\"transaction_value\"] - df_test_droped[\"total_price\"]\n",
    "\n",
    "df_test_droped[\"placed_year\"] = df_test_droped[\"placed_time\"].dt.year\n",
    "df_test_droped[\"placed_month\"] = df_test_droped[\"placed_time\"].dt.month\n",
    "df_test_droped[\"placed_weekday\"] = df_test_droped[\"placed_time\"].dt.weekday\n",
    "df_test_droped[\"placed_hour\"] = df_test_droped[\"placed_time\"].dt.hour\n",
    "df_test_droped[\"placed_minute\"] = df_test_droped[\"placed_time\"].dt.minute\n",
    "\n",
    "df_test_droped[\"latitude_dis\"] = abs(df_test_droped[\"driver_latitude_pick_up\"] - df_test_droped[\"driver_latitude_complete\"])\n",
    "df_test_droped[\"longitude_dis\"] = abs(df_test_droped[\"driver_longitude_pick_up\"] - df_test_droped[\"driver_longitude_complete\"])\n",
    "\n",
    "columns_to_cut = [\"driver_latitude_pick_up\", \"driver_longitude_pick_up\", \"driver_latitude_complete\", \"driver_longitude_complete\"]\n",
    "\n",
    "for i in columns_to_cut:\n",
    "    df_test_droped[i] = pd.qcut(df_test_droped[i],50)\n",
    "    \n",
    "    \n",
    "df_test_droped[\"placed_minute\"] = pd.qcut(df_test_droped[\"placed_minute\"], 6)\n",
    "    \n",
    "#impute na with 0 or mean for some continues\n",
    "df_test_droped[[\"subsidy\", \"adjustment\"]] = df_test_droped[[\"subsidy\", \"adjustment\"]].fillna(0)\n",
    "df_test_droped[\"dis\"] = np.sqrt(df_test_droped[\"latitude_dis\"]**2 + df_test_droped[\"longitude_dis\"]**2)\n",
    "#delete the outlier value\n",
    "df_test_droped.loc[df_test_droped[\"dis\"] >1, \"dis\"] = np.nan\n",
    "df_test_droped[\"dis\"] = df_test_droped[\"dis\"].fillna(np.mean(df_test_droped[\"dis\"]))\n",
    "\n",
    "df_test_droped = df_test_droped.drop([\"order_status\", \"service_type\", \"order_date_time\", \"cs_comment\", \"cancel_type\", \"cancel_reason\",\n",
    "              \"cancelled_by\", \"cancelled_time\", \"is_responded\", \"reject_time\", \"arrival_time_geofence\",\n",
    "              \"order_type\", \"driver_latitude_take_order\", \"driver_longitude_take_order\", \"pathcomma\", \"path\", \n",
    "              \"placed_time\",\"total_price_+_tips\", \"latitude_dis\", \"longitude_dis\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>final_subset</th>\n",
       "      <th>service_type</th>\n",
       "      <th>special_request</th>\n",
       "      <th>promo_code</th>\n",
       "      <th>payment_method</th>\n",
       "      <th>total_price</th>\n",
       "      <th>adjustment</th>\n",
       "      <th>transaction_value</th>\n",
       "      <th>corporate_code</th>\n",
       "      <th>...</th>\n",
       "      <th>promo_code_int</th>\n",
       "      <th>path_from_int</th>\n",
       "      <th>path_to_int</th>\n",
       "      <th>placed_year_int</th>\n",
       "      <th>placed_month_int</th>\n",
       "      <th>order_type_int</th>\n",
       "      <th>service_type_int</th>\n",
       "      <th>placed_weekday_int</th>\n",
       "      <th>placed_hour_int</th>\n",
       "      <th>placed_minute_int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1962533</td>\n",
       "      <td>A</td>\n",
       "      <td>TON55</td>\n",
       "      <td>TAILBOARD</td>\n",
       "      <td>nan</td>\n",
       "      <td>Cash</td>\n",
       "      <td>175</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.671535</td>\n",
       "      <td>EVCS</td>\n",
       "      <td>...</td>\n",
       "      <td>1081</td>\n",
       "      <td>56</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1638808</td>\n",
       "      <td>A</td>\n",
       "      <td>VAN</td>\n",
       "      <td>SIXFT</td>\n",
       "      <td>nan</td>\n",
       "      <td>Cash</td>\n",
       "      <td>75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.667038</td>\n",
       "      <td>EVCS</td>\n",
       "      <td>...</td>\n",
       "      <td>1081</td>\n",
       "      <td>10</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1784143</td>\n",
       "      <td>A</td>\n",
       "      <td>VAN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>Cash</td>\n",
       "      <td>75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.667038</td>\n",
       "      <td>nan</td>\n",
       "      <td>...</td>\n",
       "      <td>1081</td>\n",
       "      <td>120</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1687581</td>\n",
       "      <td>A</td>\n",
       "      <td>VAN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>Cash</td>\n",
       "      <td>130</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.069177</td>\n",
       "      <td>EVCS</td>\n",
       "      <td>...</td>\n",
       "      <td>1081</td>\n",
       "      <td>8</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1481932</td>\n",
       "      <td>A</td>\n",
       "      <td>VAN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>Cash</td>\n",
       "      <td>55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.934753</td>\n",
       "      <td>EVCS</td>\n",
       "      <td>...</td>\n",
       "      <td>1081</td>\n",
       "      <td>46</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   order_id final_subset service_type special_request promo_code  \\\n",
       "0   1962533            A        TON55       TAILBOARD        nan   \n",
       "2   1638808            A          VAN           SIXFT        nan   \n",
       "3   1784143            A          VAN             NaN        nan   \n",
       "4   1687581            A          VAN             NaN        nan   \n",
       "5   1481932            A          VAN             NaN        nan   \n",
       "\n",
       "  payment_method  total_price  adjustment  transaction_value corporate_code  \\\n",
       "0           Cash          175         NaN           0.671535           EVCS   \n",
       "2           Cash           75         NaN          -0.667038           EVCS   \n",
       "3           Cash           75         NaN          -0.667038            nan   \n",
       "4           Cash          130         NaN           0.069177           EVCS   \n",
       "5           Cash           55         NaN          -0.934753           EVCS   \n",
       "\n",
       "         ...         promo_code_int  path_from_int path_to_int  \\\n",
       "0        ...                   1081             56          36   \n",
       "2        ...                   1081             10         159   \n",
       "3        ...                   1081            120          49   \n",
       "4        ...                   1081              8         144   \n",
       "5        ...                   1081             46          59   \n",
       "\n",
       "   placed_year_int  placed_month_int order_type_int service_type_int  \\\n",
       "0                1                 4              2                3   \n",
       "2                0                 1              2                4   \n",
       "3                0                 3              2                4   \n",
       "4                0                 2              2                4   \n",
       "5                0                 9              2                4   \n",
       "\n",
       "  placed_weekday_int  placed_hour_int  placed_minute_int  \n",
       "0                  4                3                  1  \n",
       "2                  2                7                  0  \n",
       "3                  5                5                  2  \n",
       "4                  2                6                  1  \n",
       "5                  5                2                  1  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#encode special_request\n",
    "df_request = pd.DataFrame(data = {\"request\" :df_droped.special_request.str.cat(sep=',').split(\",\")})\n",
    "unique_request = df_request.request.unique()\n",
    "\n",
    "for i in unique_request:\n",
    "    df_droped[i] = df_droped.special_request.str.contains(i).fillna(0)\n",
    "\n",
    "df_droped[unique_request] = df_droped[unique_request].astype(int)\n",
    "\n",
    "for i in unique_request:\n",
    "    df_test_droped[i] = df_test_droped.special_request.str.contains(i).fillna(0)\n",
    "\n",
    "df_test_droped[unique_request] = df_test_droped[unique_request].astype(int)\n",
    "\n",
    "categorical_headers = [\"final_subset\", \"payment_method\", \"corporate_code\", \"client_id\", \"promo_code\",\n",
    "                       \"path_from\", \"path_to\", \"placed_year\", \"placed_month\", \"order_type\", \"service_type\",\n",
    "                       \"placed_weekday\", \"placed_hour\", \"placed_minute\"]\n",
    "df_droped[categorical_headers] = df_droped[categorical_headers].astype(str)\n",
    "df_test_droped[categorical_headers] = df_test_droped[categorical_headers].astype(str)\n",
    "\n",
    "# define objects that can encode each variable as integer    \n",
    "encoders = dict() \n",
    "\n",
    "# train all encoders (special case the target 'income')\n",
    "for col in categorical_headers+[\"target\"]:\n",
    "    \n",
    "    if col in [\"target\"]:\n",
    "        tmp = LabelEncoder()\n",
    "        df_droped[col] = tmp.fit_transform(df_droped[col])\n",
    "    else:\n",
    "        # integer encoded variables\n",
    "        df_droped[col] = df_droped[col].str.strip()\n",
    "        encoders[col] = LabelEncoder()\n",
    "        df_droped[col + \"_int\"] = encoders[col].fit_transform(df_droped[col])\n",
    "        \n",
    "for col in categorical_headers:\n",
    "        # integer encoded variables\n",
    "        df_test_droped[col] = df_test_droped[col].str.strip()\n",
    "        encoders[col] = LabelEncoder()\n",
    "        df_test_droped[col + \"_int\"] = encoders[col].fit_transform(df_test_droped[col])\n",
    "\n",
    "# scale the numeric, continuous variables\n",
    "numeric_headers = [\"transaction_value\", \"taken_minutes\", \"pickup_minutes\"]\n",
    "\n",
    "\n",
    "for col in numeric_headers:\n",
    "    df_droped[col] = df_droped[col].astype(np.float)\n",
    "    \n",
    "    ss = StandardScaler()\n",
    "    df_droped[col] = ss.fit_transform(df_droped[col].values.reshape(-1, 1))\n",
    "    \n",
    "for col in numeric_headers:\n",
    "    df_test_droped[col] = df_test_droped[col].astype(np.float)\n",
    "    \n",
    "    ss = StandardScaler()\n",
    "    df_test_droped[col] = ss.fit_transform(df_test_droped[col].values.reshape(-1, 1))\n",
    "        \n",
    "df_droped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "categorical_headers_ints = [x+'_int' for x in categorical_headers] + [\"SIXFT\", \"PET\", \"NEWCAR\", \"TROLLEY3\", \"ONSITE\",\n",
    "                                                                      \"OLD_CARS\", \"HOURLY\", \"ENGLISH\", \"STANDARD_MOVING\",\n",
    "                                                                      \"VITO\", \"EXHIBIT\"]\n",
    "categorical_headers_ints = list(set(categorical_headers_ints).difference(set([\"client_id_int\", \"promo_code_int\", \"corporate_code_int\"])))\n",
    "\n",
    "\n",
    "df_train, df_validation = train_test_split(df_droped, test_size=0.2)\n",
    "df_test = df_test_droped\n",
    "\n",
    "y_train = df_train[\"target\"]\n",
    "y_validation = df_validation[\"target\"]\n",
    "\n",
    "#Save off the numeric features\n",
    "X_train_num =  df_train[numeric_headers].values\n",
    "X_validation_num = df_validation[numeric_headers].values\n",
    "X_test_num =  df_test[numeric_headers].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Input, Dropout\n",
    "from keras.layers import Embedding, Flatten, concatenate\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_columns = [[\"path_from\",\"path_to\"],\n",
    "                 #[\"driver_latitude_pick_up\", \"driver_longitude_pick_up\"],\n",
    "                 #[\"driver_latitude_complete\", \"driver_longitude_complete\"],\n",
    "                 [\"placed_year\", \"placed_month\"],\n",
    "                 [\"placed_month\", \"placed_weekday\", \"placed_hour\"],\n",
    "                 [\"placed_weekday\", \"placed_hour\", \"placed_minute\"],\n",
    "                 [\"order_type\", \"placed_hour\",\"placed_weekday\"],\n",
    "                 [\"order_type\", \"service_type\"],\n",
    "                 [\"pathstops\"],\n",
    "                 [\"client_id\"],\n",
    "                 [\"promo_code\"],\n",
    "                 [\"corporate_code\"]]\n",
    "\n",
    "embed_branches = []\n",
    "X_ints_train = []\n",
    "X_ints_validation = []\n",
    "X_ints_test = []\n",
    "all_inputs = []\n",
    "all_wide_branch_outputs = []\n",
    "all_deep_branch_outputs = []\n",
    "\n",
    "for cols in cross_columns:\n",
    "    # encode crossed columns as ints for the embedding\n",
    "    enc = LabelEncoder()\n",
    "    \n",
    "    # create crossed labels\n",
    "    if len(cols) > 1 :\n",
    "        X_crossed_train = df_train[cols].apply(lambda x: \"_\".join(x), axis=1)\n",
    "        X_crossed_validation = df_validation[cols].apply(lambda x: \"_\".join(x), axis=1)\n",
    "        X_crossed_test = df_test[cols].apply(lambda x: \"_\".join(x), axis=1)\n",
    "    else:\n",
    "        X_crossed_train = df_train[cols].iloc[:,0].astype(\"str\")\n",
    "        X_crossed_validation = df_validation[cols].iloc[:,0].astype(\"str\")\n",
    "        X_crossed_test = df_test[cols].iloc[:,0].astype(\"str\")\n",
    "    \n",
    "    enc.fit(np.hstack((X_crossed_train.values, X_crossed_validation.values, X_crossed_test.values)))\n",
    "    X_crossed_train = enc.transform(X_crossed_train)\n",
    "    X_crossed_validation = enc.transform(X_crossed_validation)\n",
    "    X_crossed_test = enc.transform(X_crossed_test)\n",
    "    X_ints_train.append( X_crossed_train )\n",
    "    X_ints_validation.append( X_crossed_validation )\n",
    "    X_ints_test.append( X_crossed_test )\n",
    "    \n",
    "    # get the number of categories\n",
    "    N = max(X_ints_train[-1]+2)\n",
    "    \n",
    "    # create embedding branch from the number of categories\n",
    "    inputs = Input(shape=(1,),dtype=\"int32\", name = \"_\".join(cols))\n",
    "    all_inputs.append(inputs)\n",
    "    x = Embedding(input_dim=N, \n",
    "                  output_dim=int(np.log2(N)), \n",
    "                  input_length=1, name = \"_\".join(cols)+\"_embed\")(inputs)\n",
    "    x = Flatten()(x)\n",
    "    all_wide_branch_outputs.append(x)\n",
    "        \n",
    "# merge the branches together\n",
    "wide_branch = concatenate(all_wide_branch_outputs, name=\"wide_concat\")\n",
    "wide_branch = Dropout(0.3)(wide_branch)\n",
    "wide_branch = Dense(units=400,activation=\"relu\",name=\"wide_combined_1\")(wide_branch)\n",
    "wide_branch = Dropout(0.3)(wide_branch)\n",
    "#wide_branch = Dense(units=200,activation=\"relu\",name=\"wide_combined_2\")(wide_branch)\n",
    "#wide_branch = Dropout(0.3)(wide_branch)\n",
    "wide_branch = Dense(units=2,activation=\"sigmoid\",name=\"wide_combined\")(wide_branch)\n",
    "\n",
    "\n",
    "# reset this input branch\n",
    "all_deep_branch_outputs = []\n",
    "# add in the embeddings\n",
    "for col in categorical_headers_ints:\n",
    "    # encode as ints for the embedding\n",
    "    X_ints_train.append( df_train[col].values )\n",
    "    X_ints_validation.append( df_validation[col].values )\n",
    "    X_ints_test.append( df_test[col].values )\n",
    "    \n",
    "    # get the number of categories\n",
    "    N = max(df_droped[col]) + 1\n",
    "    \n",
    "    # create embedding branch from the number of categories\n",
    "    inputs = Input(shape=(1,),dtype=\"int32\", name=col)\n",
    "    all_inputs.append(inputs)\n",
    "    x = Embedding(input_dim=N, \n",
    "                  output_dim=int(np.sqrt(N)), \n",
    "                  input_length=1, name=col+\"_embed\")(inputs)\n",
    "    x = Flatten()(x)\n",
    "    all_deep_branch_outputs.append(x)\n",
    "    \n",
    "# also get a dense branch of the numeric features\n",
    "all_inputs.append(Input(shape=(X_train_num.shape[1],),\n",
    "                        sparse=False,\n",
    "                        name=\"numeric_data\"))\n",
    "\n",
    "x = all_inputs[-1]\n",
    "#x = Dropout(0.15)(all_inputs[-1])\n",
    "x = Dense(units=30, activation=\"relu\",name=\"numeric_1\")(x)\n",
    "\n",
    "all_deep_branch_outputs.append( x )\n",
    "\n",
    "# merge the deep branches together\n",
    "deep_branch = concatenate(all_deep_branch_outputs,name=\"concat_embeds\")\n",
    "#deep_branch = Dropout(0.2)(deep_branch)\n",
    "deep_branch = Dense(units=200,activation=\"relu\", name=\"deep1\")(deep_branch)\n",
    "#deep_branch = Dropout(0.2)(deep_branch)\n",
    "deep_branch = Dense(units=100,activation=\"relu\", name=\"deep2\")(deep_branch)\n",
    "#deep_branch = Dropout(0.2)(deep_branch)\n",
    "deep_branch = Dense(units=50,activation=\"relu\", name=\"deep3\")(deep_branch)\n",
    "#deep_branch = Dropout(0.2)(deep_branch)\n",
    "deep_branch = Dense(units=25,activation=\"relu\", name=\"deep4\")(deep_branch)\n",
    "#deep_branch = Dropout(0.2)(deep_branch)\n",
    "deep_branch = Dense(units=10,activation=\"relu\", name=\"deep5\")(deep_branch)\n",
    "#deep_branch = Dropout(0.2)(deep_branch)\n",
    "deep_branch = Dense(units=25,activation=\"relu\", name=\"deep6\")(deep_branch)\n",
    "#eep_branch = Dropout(0.2)(deep_branch)\n",
    "deep_branch = Dense(units=50,activation=\"relu\", name=\"deep7\")(deep_branch)\n",
    "   \n",
    "final_branch = concatenate([wide_branch, deep_branch],name=\"concat_deep_wide\")\n",
    "#final_branch = Dropout(0.2)(final_branch)\n",
    "final_branch = Dense(units=100,activation=\"relu\",name=\"combined_1\")(final_branch)\n",
    "#final_branch = Dropout(0.2)(final_branch)\n",
    "#final_branch = Dense(units=50,activation=\"relu\",name=\"combined_2\")(final_branch)\n",
    "#final_branch = Dropout(0.2)(final_branch)\n",
    "final_branch = Dense(units=1,activation=\"sigmoid\",name=\"combined_deep\")(final_branch)\n",
    "\n",
    "model = Model(inputs=all_inputs, outputs=final_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "\"dot.exe\" not found in path.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mE:\\ana\\envs\\tensorflow\\lib\\site-packages\\pydot.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(self, prog, format)\u001b[0m\n\u001b[0;32m   1877\u001b[0m                 \u001b[0mshell\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1878\u001b[1;33m                 stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n\u001b[0m\u001b[0;32m   1879\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ana\\envs\\tensorflow\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds)\u001b[0m\n\u001b[0;32m    675\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[0;32m    677\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ana\\envs\\tensorflow\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m    956\u001b[0m                                          \u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 957\u001b[1;33m                                          startupinfo)\n\u001b[0m\u001b[0;32m    958\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] 系统找不到指定的文件。",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-349-47504ae4d3ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSVG\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvis_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mSVG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_to_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'dot'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'svg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\ana\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[1;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[0m_check_pydot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m     \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mdot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rankdir'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ana\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m# Attempt to create an image of a blank graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# to check the pydot/graphviz installation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         raise OSError(\n",
      "\u001b[1;32mE:\\ana\\envs\\tensorflow\\lib\\site-packages\\pydot.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(self, prog, format)\u001b[0m\n\u001b[0;32m   1881\u001b[0m                 raise Exception(\n\u001b[0;32m   1882\u001b[0m                     '\"{prog}\" not found in path.'.format(\n\u001b[1;32m-> 1883\u001b[1;33m                         prog=prog))\n\u001b[0m\u001b[0;32m   1884\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1885\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: \"dot.exe\" not found in path."
     ]
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 372397 samples, validate on 93100 samples\n",
      "Epoch 1/5\n",
      "372397/372397 [==============================] - 34s 92us/step - loss: 0.5112 - acc: 0.7527 - mean_squared_error: 0.1702 - val_loss: 0.4626 - val_acc: 0.7877 - val_mean_squared_error: 0.1506\n",
      "Epoch 2/5\n",
      "372397/372397 [==============================] - 30s 81us/step - loss: 0.4555 - acc: 0.7919 - mean_squared_error: 0.1482 - val_loss: 0.4297 - val_acc: 0.8083 - val_mean_squared_error: 0.1384\n",
      "Epoch 3/5\n",
      "372397/372397 [==============================] - 30s 81us/step - loss: 0.4334 - acc: 0.8050 - mean_squared_error: 0.1399 - val_loss: 0.4130 - val_acc: 0.8131 - val_mean_squared_error: 0.1324\n",
      "Epoch 4/5\n",
      "372397/372397 [==============================] - 30s 81us/step - loss: 0.4250 - acc: 0.8090 - mean_squared_error: 0.1369 - val_loss: 0.4041 - val_acc: 0.8237 - val_mean_squared_error: 0.1289\n",
      "Epoch 5/5\n",
      "372397/372397 [==============================] - 30s 81us/step - loss: 0.4176 - acc: 0.8132 - mean_squared_error: 0.1342 - val_loss: 0.3926 - val_acc: 0.8259 - val_mean_squared_error: 0.1252\n",
      "Wall time: 2min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model.compile(optimizer='Adadelta',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc', 'mse'])\n",
    "\n",
    "# lets also add the history variable to see how we are doing\n",
    "# and lets add a validation set to keep track of our progress\n",
    "\n",
    "history = model.fit(X_ints_train+ [X_train_num],\n",
    "                    y_train, \n",
    "                    epochs=5, \n",
    "                    batch_size=256, \n",
    "                    verbose=1, \n",
    "                    validation_data = (X_ints_validation + [X_validation_num], y_validation),\n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', patience=2)])\n",
    "#callbacks=[EarlyStopping(monitor='val_loss', patience=4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[51294  5057]\n",
      " [11156 25593]] \n",
      " acc: 0.8258539205155746 \n",
      " f1_score: 0.759447469547026 \n",
      " brier_score: 0.12515882217307114\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics as mt\n",
    "\n",
    "y_prob =model.predict(X_ints_validation + [X_validation_num])\n",
    "\n",
    "yhat = y_prob > 0.5\n",
    "                \n",
    "print(mt.confusion_matrix(y_validation,yhat), \"\\n\", \"acc:\", mt.accuracy_score(y_validation,yhat), \n",
    "      \"\\n\", \"f1_score:\", mt.f1_score(y_validation,yhat), \"\\n\", \"brier_score:\", mt.brier_score_loss(y_validation,y_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_prob =model.predict(X_ints_test + [X_test_num])\n",
    "\n",
    "yhat_test = y_test_prob > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_predict = pd.DataFrame()\n",
    "df_predict[\"order_id\"] = df_test[\"order_id\"]\n",
    "df_predict[\"y_prob\"] = y_test_prob\n",
    "df_predict[\"yhat\"] = yhat_test\n",
    "\n",
    "df_predict.to_csv(\"E:/python/testproject/test_withtime_60.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '\\ufeff2716182'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-97991661610f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_tmp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"order_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '\\ufeff2716182'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
